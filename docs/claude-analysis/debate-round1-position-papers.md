# Round 1: Initial Position Papers

## 1. Advocate [A1-A5]

### The Authority Gap Is Real and It Matters

The most important finding in the Telos evaluation is not that the Telos+Git agent detected more issues than the Git-only agent — in most experiments, both agents noticed the same problems. The critical finding is what happened *after* detection. In Experiments C, E, and F, the Git-only agent identified concerning patterns (the token expiry change, the validation removal, the verbose error messages) but ultimately **approved** the changes. The Telos agent, given the same diffs and the same surface-level signals, **rejected** them. This is the "authority gap," and it maps directly to a well-documented failure mode in real-world code review: the "approve with comments" trap, where reviewers defer to the author's stated rationale when they lack an authoritative external reference to anchor their objection. In Experiment C, the Git-only agent found the inline code comment "CONSTRAINT: must be <= 1 hour" but dismissed it as informal. In Experiment F, it recognized the potential for information leakage but accepted the commit message's framing of "easier debugging." Telos converted these soft concerns into hard rejections by providing constraints as first-class, recorded project artifacts rather than scattered code comments that can be rationalized away. This is not a marginal improvement — it is a categorical difference in outcome: approve versus reject, vulnerability shipped versus vulnerability caught.

### Structural Advantages That Scale Beyond the Evaluation

The content-addressable architecture of Telos (SHA-256 hashing, immutable DAG, canonical serialization, fan-out storage) is borrowed from Git's proven design but applied to a fundamentally different domain: intent rather than code. This design choice has compounding benefits that the current evaluation, which is necessarily small-scale, only begins to demonstrate. Constraints in Telos are queryable first-class objects, not grep-dependent code comments. The command `telos query intents --constraint-contains "must not" --json` returns every safety constraint in the project as structured data, filterable by impact area, linkable to decisions and behavior specs. This is qualitatively different from `grep -r "CONSTRAINT"` or relying on commit messages to contain the right keywords. Experiment D demonstrates this concretely: the auth RBAC-to-task permission link was surfaced by `telos context --impact tasks` but missed entirely by grep-based analysis, because the relationship between the auth module's RBAC roles and the tasks module's permission model was encoded in impact tags, not in textual co-occurrence. As projects grow and constraints accumulate, the structured queryability of Telos becomes increasingly valuable relative to the linear degradation of grep-through-history approaches. The stream abstraction and DAG structure also provide a natural foundation for future capabilities: constraint conflict detection across streams, automated pre-commit constraint validation, and multi-agent workflows where agents share a structured intent context rather than passing unstructured text.

### Honest Assessment of Limitations

The evaluation has real weaknesses that should be acknowledged. The agents are simulated — the "responses" are hand-crafted to represent plausible LLM behavior, not actual outputs from running Claude or GPT-4 against the contexts. The scoring relies on keyword matching, which can both over-credit superficial mentions and miss semantically correct answers phrased differently. The evaluation uses a single small application (TaskBoard) with ~15 constraints, and N=1 for every experiment. Experiment A's failure (5% delta vs. the 30% threshold) shows that when commit messages are detailed enough, the gap narrows considerably. These are real limitations. But they are limitations of the *evaluation methodology*, not of the *core mechanism*. The authority gap observed in C/E/F — where structured constraints changed the agent's decision from approve to reject — is a behavioral phenomenon that holds regardless of whether keyword scoring captures it perfectly. The simulated responses are designed to be conservative and plausible (the Git-only agent's responses are nuanced and technically competent, not straw men). And the single-application scope, while narrow, is sufficient to demonstrate the mechanism; generalization is a matter of scale testing, not of whether the mechanism exists.

### Numbered Claims

**[A1]** The "authority gap" demonstrated in Experiments C, E, and F — where both agents detected the same issue but only the Telos agent rejected the change — represents a genuine and practically significant advantage. Structured constraints function as authoritative project law that agents can cite, whereas code comments and commit messages function as advisory suggestions that agents rationalize away under pressure from plausible commit messages.

**[A2]** Telos's queryable constraint model (first-class objects with impact tags, constraint text, and behavior specs) is structurally superior to the alternatives (code comments, commit message conventions, external documentation) for AI agent consumption. The `telos context --impact <area> --json` command provides exactly the kind of structured, scoped, machine-readable context that LLM agents need, and nothing in the Git ecosystem provides an equivalent.

**[A3]** The content-addressable, immutable DAG architecture is a sound engineering foundation that enables capabilities beyond what the current evaluation tests: constraint conflict detection, intent history traversal, stream-based branching of intent, and verifiable integrity of the constraint record. The 59 passing tests across three crates demonstrate that the implementation is solid, not just the concept.

**[A4]** Experiment A's failure actually strengthens the overall case by demonstrating intellectual honesty and identifying the boundary condition: Telos's advantage diminishes when commit messages are unusually detailed. This correctly predicts that Telos's greatest value will be in real-world projects where commit messages are terse, inconsistent, or absent — which describes the overwhelming majority of software projects.

**[A5]** The acknowledged limitations (simulated agents, keyword scoring, N=1, single application) are all addressable through straightforward follow-up work (live LLM evaluation, semantic scoring, statistical replication, multi-project testing) and do not invalidate the core finding. The authority gap mechanism is robust: providing agents with structured, authoritative constraints changes their behavior from "approve with comments" to "reject with citation," and this behavioral difference persists regardless of the specific scoring methodology used to measure it.

---

## 2. Skeptic [S1-S5]

### The Central Problem

The central problem with Telos's validation is that it proves only that the developer can write two different strings and then score them differently — it does not prove that AI agents, given Telos context, actually behave differently. The entire experimental pipeline in `run_scoring.py` (lines 20-183) consists of hand-authored simulated responses: multi-paragraph "agent reviews" written by the same person who designed the scoring rubrics and the constraints being tested. The git-only agent's response in Experiment C conveniently says "APPROVE with minor comment" while the Telos agent's response says "REJECT — Constraint Violation Detected." These are not agent outputs — they are the developer's hypothesis about what agents *would* say, presented as if they were measured outcomes. The EVALUATION.md document describes this as "simulated agents" (Section 8, Limitations), but the Results section (Section 7) reports scores with the assertiveness of empirical findings: "6 of 7 experiments passed (Strong Pass)." This framing mismatch between what was actually done (writing two different strings) and what is claimed (agents equipped with Telos make better decisions) is the evaluation's most fundamental weakness.

### Scoring and Alternative Explanations

Even accepting the simulated framework at face value, the scoring methodology is unreliable in both directions. The keyword-matching rubrics (`evaluate_review_response`, `evaluate_status_response`, etc.) use substring containment checks like `"violat" in response_lower` or `"reject" in response_lower`. This means a response stating "I see no violation here and would not reject this change" would score `caught_violation=True` and `recommended_rejection=True`. The evaluation has no negative-case testing: it never presents a benign change to either agent to check whether the Telos agent over-rejects. If the Telos agent simply prepended "REJECT. Constraint violation." to every review, it would achieve perfect scores across all experiments. Meanwhile, the one experiment that did fail — Experiment A, where good commit messages alone achieved 95% recall — actually undermines the core thesis. It suggests that the cheaper, simpler intervention of writing better commit messages captures most of the value Telos claims to provide.

### Missing Comparisons

The validation sidesteps the most important practical questions: cost, friction, and alternatives. Telos requires developers to manually record intents, constraints, and decisions using a separate CLI tool alongside their normal git workflow. The README shows commands like `telos intent --statement "..." --constraint "..." --constraint "..." --impact auth --impact security --behavior "GIVEN...|WHEN...|THEN..."`. This is substantial ceremony. The evaluation never compares Telos against lighter-weight alternatives that capture similar information: ADRs (Architecture Decision Records) stored as markdown files in the repo, PR templates with constraint checklists, or even just enforced commit message conventions. Any of these could be parsed by agents with existing tooling. The evaluation also never addresses what happens when constraints become stale, contradictory, or wrong — a problem that grows with project size and time, exactly the scenarios where Telos claims to add the most value.

### Challenges

**[S1] The experiments do not measure agent behavior.** All seven experiments use developer-authored simulated responses, not actual LLM outputs. The git-only responses are written to miss violations; the Telos responses are written to catch them. This is hypothesis illustration, not hypothesis testing. The "6 of 7 pass" result is predetermined by the strings the developer chose to write. Until these experiments are run with real LLM agents (as the Limitations section itself acknowledges is needed), the claimed results have no empirical standing.

**[S2] Keyword scoring is unreliable and gameable.** The scoring functions use naive substring matching (e.g., `"violat"` matches "no violation found," `"reject"` matches "I would not reject this"). There are no checks for negation, context, or semantic meaning. Furthermore, the Telos agent's simulated responses appear to be written with awareness of the exact keywords the scoring functions check for — the response in Experiment F, for instance, contains the exact phrase "must not leak" which is one of the scoring indicators in `evaluate_leak_response`. The developer who wrote the responses also wrote the rubrics.

**[S3] Experiment A's failure undermines the value proposition.** The git-only agent scored 95% overall recall using only commit messages, versus 100% for the Telos agent — a 5% delta against a 30% threshold. The evaluation's own notes state: "Git commit messages in this project are unusually detailed." But this is not unusual — it is what good engineering practice looks like. If the argument is "Telos helps when commit messages are bad," then the tool is compensating for a process failure that could be fixed more cheaply by enforcing commit message quality. The evaluation never tests against realistic (i.e., mediocre) commit messages to demonstrate the claimed gap.

**[S4] There is no false-positive testing.** Not a single experiment presents a benign, constraint-respecting change to the agents. We have no data on whether the Telos agent would incorrectly reject valid changes by pattern-matching on stale constraints, misinterpreting constraint scope, or simply being primed by the more adversarial task prompt it receives ("Check it against the recorded constraints... Identify any constraint violations"). The Telos agent's task description explicitly tells it to look for violations, while the git-only agent gets a neutral "Identify any issues." This prompt asymmetry alone could explain the behavioral difference without any contribution from Telos data.

**[S5] Simpler alternatives are never compared.** The evaluation positions the choice as "Git alone vs. Git + Telos" but never considers Git + ADRs, Git + PR templates with constraint checklists, Git + automated linting rules, or Git + structured commit messages with conventional-commits enforcement. All of these are established, low-friction practices that capture constraint and decision information within the existing toolchain. Telos introduces a parallel data store, a new CLI, and ongoing maintenance burden. The evaluation must show that Telos outperforms these cheaper alternatives to justify its additional complexity — and it does not attempt this comparison.

---

## 3. Empiricist [E1-E5]

### Assessment

The Telos evaluation framework presents a structurally interesting hypothesis — that structured intent context improves AI agent decision-making relative to Git history alone — but the evidence offered to support that hypothesis fails to meet basic standards of empirical rigor. The core problem is not that the results are wrong, but that the methodology cannot distinguish between "Telos works" and "the evaluation was designed to produce the desired outcome." Every experiment runs exactly one hand-authored response per condition, scored by keyword substring matching, against scenarios the evaluator constructed. There is no independent measurement, no variance, no blinding, and no adversarial testing. The EVALUATION.md document is commendably transparent about several of these limitations (Section 8), but the summary and README still present "6 of 7 passed (Strong Pass)" as though it were a validated empirical finding rather than a demonstration sketch.

The most revealing structural issue is the asymmetry in the experimental design itself. The Telos+Git agent receives a different task prompt than the Git-only agent — the Telos prompt explicitly instructs the agent to "Check it against the recorded constraints and intents. Identify any constraint violations." (visible in review_test.py:65-66, status_test.py:57-59, leak_test.py:66-68, escalation_test.py:82-84). The Git-only agent receives the generic "Identify any issues." This is not testing whether Telos context helps an agent; it is testing whether an agent told to look for constraint violations finds constraint violations. A fair comparison would give both agents identical instructions and let the tool availability alone be the independent variable.

The claim structure of the evaluation has a deeper circularity problem: the person who wrote the intents and constraints also wrote the simulated agent responses and also wrote the keyword-matching rubrics. This is analogous to a teacher writing the test questions, writing the student answers, and then grading them — all as one person. There is no point in this pipeline where an independent signal enters the system. The "results" are predetermined by authorial choices, not discovered through measurement.

### Findings

**[E1] N=1 per experiment with no variance measurement.** Each of the 7 experiments compares exactly one hand-written Git-only response against one hand-written Telos+Git response. There are no repeated trials, no confidence intervals, no effect sizes, and no way to assess whether any individual result is an outlier. The claim "6/7 experiments passed" has no statistical meaning — it is equivalent to running a clinical trial with one patient per arm and declaring the drug effective. With N=1 binary outcomes, even a coin flip has a 1-in-128 chance of producing 6/7 "successes." Rigorous evidence would require N>=30 independent LLM responses per condition, per experiment, with variance reported and statistical significance tested (e.g., McNemar's test for paired binary outcomes, or Mann-Whitney U for ordinal scores).

**[E2] Keyword-based scoring conflates lexical presence with semantic understanding.** The evaluate_*_response functions use simple substring matching: `"violat" in response_lower` (review_test.py:98), `"leak" in response_lower` (leak_test.py:99), `"remov" in response_lower` (status_test.py:89). This approach has two fatal flaws. First, it produces false positives: a response saying "there is no violation here" would score as catching the violation. Second, it produces false negatives: a semantically correct response using different phrasing would fail to score. Rigorous scoring would require either human expert raters (with inter-rater reliability measured via Cohen's kappa) or an LLM-as-judge approach with calibration against human judgments.

**[E3] All agent responses are hand-authored, not generated by real LLMs.** The file `run_scoring.py` contains the complete text of every "agent response" as Python string literals (lines 20-183). These are not outputs from Claude, GPT-4, or any other model — they are prose written by the evaluator to represent what an idealized agent might say. The entire claimed advantage — that Telos agents reject while Git-only agents approve — is an artifact of authorial intent, not an observed behavioral difference.

**[E4] No false-positive (specificity) measurement.** All 7 experiments test a single direction: "does the Telos agent catch something bad?" Zero experiments test the complementary question: "does the Telos agent incorrectly reject a good change?" A system that rejects everything would score perfectly on every experiment except A.

**[E5] Prompt asymmetry confounds tool availability with instruction differences.** The Git-only prompt says: "Review this code change. Identify any issues, regressions, or concerns." The Telos+Git prompt says: "Review this code change. Check it against the recorded constraints and intents. Identify any constraint violations." The current design tests the combined effect of (Telos data + violation-hunting instructions) vs. (Git data + generic instructions), making it impossible to isolate Telos's contribution.

---

## 4. Practitioner [P1-P5]

### Opening Assessment

Telos solves a real problem that most experienced developers have felt: the slow decay of institutional knowledge as teams grow and members rotate. The idea of a structured, queryable intent layer alongside Git is intellectually compelling, and the validation experiments demonstrate a genuine signal. The "authority gap" finding — where agents detect problems but lack the structured backing to enforce rejection — is a real insight about how AI-assisted code review fails today.

### Adoption Barriers

However, as someone who would need to champion this tool in an actual engineering team, the adoption barriers are formidable. Telos asks developers to maintain a parallel metadata system alongside Git, with its own commands, its own mental model, and its own discipline requirements. The CLI is not burdensome on a per-command basis, but the aggregate workflow cost is substantial: for every meaningful code change, developers are now expected to run `telos intent` before (or alongside) their normal git workflow. This is not a linter that runs automatically — it requires active, deliberate participation from every developer on every change that carries design intent. The history of developer tooling shows that tools requiring manual discipline at this granularity either get automated away or die from neglect. And Telos currently has no automation layer — no git hooks, no IDE integration, no way to infer intents from code changes.

### Production Readiness

The validation framework operates in conditions that will never hold in production. The experiments use a purpose-built 3-module application with 15 intents where every single constraint was carefully authored by the evaluator. In a real 50-developer team working on a service with hundreds of modules, the constraint coverage will be spotty, the intents will be written with varying quality, and the staleness problem will compound rapidly since intents are immutable once recorded.

### Concerns

**[P1] CLI friction and workflow tax.** Every intent requires `--statement`, `--constraint`, and `--impact` flags, each of which demands the developer stop and articulate their purpose in a structured way. Compare to existing zero-overhead alternatives: a well-written commit message, a PR description template, or an ADR file. Developers already resist writing thorough commit messages; asking them to also run a separate command with three to four flags will face even stronger resistance. The roadmap mentions git hook integration and IDE extensions in Phase 3, but Phase 3 is speculative. Today, the tool requires pure manual discipline, and manual discipline does not scale.

**[P2] Intent staleness and immutability problem.** All Telos objects are immutable and content-addressed. Once an intent is recorded, it cannot be updated or deprecated. After 6 months of refactoring, the intent store will contain references to code structures and constraint contexts that no longer exist. Stale intents are arguably worse than no intents: an AI agent querying `telos context --impact auth` six months from now may get constraints that reference a rewritten module, and those constraints will carry the same "structured authority." The authority gap works both ways — false authority from stale constraints could cause agents to reject valid changes.

**[P3] Partial adoption renders the system untrustworthy.** Telos only provides value when intent coverage is comprehensive. If only 2 of 10 developers on a team use Telos, then 80% of design decisions are not captured. An agent querying for constraints will find nothing — not because there are no constraints, but because nobody recorded them. This chicken-and-egg dynamic is the adoption death spiral that kills many developer tools.

**[P4] Existing tools already cover significant ground.** ADRs capture design decisions. PR templates enforce review criteria. Code review bots check diffs against rules. Linters enforce code constraints. What Telos adds beyond these is queryable, machine-readable structure and impact-tag cross-referencing. That is genuinely novel, but the question is whether it justifies a separate system when 70% of the value could be achieved by standardizing ADRs with a consistent schema and teaching agents to parse them.

**[P5] Storage and query architecture will not scale to real teams.** Every query calls `odb.iter_all()`, which does a full filesystem walk of the `objects/` directory, deserializes every object, then filters in memory. No indexing, no caching, no pagination. At 1,000+ intents, every `telos context` or `telos query` call will walk and deserialize the entire object store. A VP of Engineering reviewing this for a 50-person team would rightly flag it as not production-ready.
